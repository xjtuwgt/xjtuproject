2020-11-06 13:02:28 INFO     CPU setting
2020-11-06 13:02:28 INFO     Loading training data...
2020-11-06 13:03:26 INFO     Loading (90447, 27) in 57.8393 seconds
2020-11-06 13:03:26 INFO     Using all training data 90447
2020-11-06 13:03:27 INFO     Loading development data...
2020-11-06 13:03:29 INFO     Loading (7405, 27) in 2.0004 seconds
2020-11-06 13:03:30 INFO     Loading data completed
2020-11-06 13:03:30 INFO     ***************************************************************************
2020-11-06 13:03:30 INFO     Loading model...
2020-11-06 13:03:34 INFO     Frozen the first 2 layers
2020-11-06 13:03:34 INFO     Loading encoder takes 3.9843
2020-11-06 13:03:34 INFO     Constructing model completes in 4.0076
2020-11-06 13:03:34 INFO     Model Parameter Configuration:
2020-11-06 13:03:34 INFO     Parameter longformer.embeddings.word_embeddings.weight: torch.Size([50265, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.embeddings.position_embeddings.weight: torch.Size([4098, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.embeddings.token_type_embeddings.weight: torch.Size([1, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.embeddings.LayerNorm.weight: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.embeddings.LayerNorm.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.self.query.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.self.key.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.self.value.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.self.query_global.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.self.query_global.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.self.key_global.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.self.key_global.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.self.value_global.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.self.value_global.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.output.dense.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.intermediate.dense.bias: torch.Size([3072]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.output.dense.weight: torch.Size([768, 3072]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.output.dense.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.output.LayerNorm.weight: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.0.output.LayerNorm.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.self.query.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.self.key.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.self.value.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.self.query_global.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.self.query_global.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.self.key_global.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.self.key_global.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.self.value_global.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.self.value_global.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.output.dense.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.intermediate.dense.bias: torch.Size([3072]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.output.dense.weight: torch.Size([768, 3072]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.output.dense.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.output.LayerNorm.weight: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.1.output.LayerNorm.bias: torch.Size([768]), require_grad = False
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.self.query.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.self.key.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.self.value.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.self.query_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.self.query_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.self.key_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.self.key_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.self.value_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.self.value_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.intermediate.dense.bias: torch.Size([3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.output.dense.weight: torch.Size([768, 3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.2.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.self.query.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.self.key.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.self.value.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.self.query_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.self.query_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.self.key_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.self.key_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.self.value_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.self.value_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.intermediate.dense.bias: torch.Size([3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.output.dense.weight: torch.Size([768, 3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.3.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.self.query.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.self.key.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.self.value.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.self.query_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.self.query_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.self.key_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.self.key_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.self.value_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.self.value_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.intermediate.dense.bias: torch.Size([3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.output.dense.weight: torch.Size([768, 3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.4.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.self.query.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.self.key.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.self.value.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.self.query_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.self.query_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.self.key_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.self.key_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.self.value_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.self.value_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.intermediate.dense.bias: torch.Size([3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.output.dense.weight: torch.Size([768, 3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.5.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.self.query.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.self.key.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.self.value.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.self.query_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.self.query_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.self.key_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.self.key_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.self.value_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.self.value_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.intermediate.dense.bias: torch.Size([3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.output.dense.weight: torch.Size([768, 3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.6.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.self.query.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.self.key.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.self.value.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.self.query_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.self.query_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.self.key_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.self.key_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.self.value_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.self.value_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.intermediate.dense.bias: torch.Size([3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.output.dense.weight: torch.Size([768, 3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.7.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.self.query.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.self.key.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.self.value.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.self.query_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.self.query_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.self.key_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.self.key_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.self.value_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.self.value_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.intermediate.dense.bias: torch.Size([3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.output.dense.weight: torch.Size([768, 3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.8.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.self.query.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.self.key.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.self.value.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.self.query_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.self.query_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.self.key_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.self.key_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.self.value_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.self.value_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.intermediate.dense.bias: torch.Size([3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.output.dense.weight: torch.Size([768, 3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.9.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.self.query.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.self.key.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.self.value.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.self.query_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.self.query_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.self.key_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.self.key_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.self.value_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.self.value_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.intermediate.dense.bias: torch.Size([3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.output.dense.weight: torch.Size([768, 3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.10.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.self.query.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.self.key.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.self.value.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.self.query_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.self.query_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.self.key_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.self.key_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.self.value_global.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.self.value_global.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.intermediate.dense.bias: torch.Size([3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.output.dense.weight: torch.Size([768, 3072]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.output.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.output.LayerNorm.weight: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encoder.layer.11.output.LayerNorm.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.pooler.dense.weight: torch.Size([768, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.pooler.dense.bias: torch.Size([768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encode_proj.weight: torch.Size([256, 768]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter longformer.encode_proj.bias: torch.Size([256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter yn_outputs.w_1.weight: torch.Size([1024, 256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter yn_outputs.w_1.bias: torch.Size([1024]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter yn_outputs.w_2.weight: torch.Size([3, 1024]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter yn_outputs.w_2.bias: torch.Size([3]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter qa_outputs.w_1.weight: torch.Size([1024, 256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter qa_outputs.w_1.bias: torch.Size([1024]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter qa_outputs.w_2.weight: torch.Size([2, 1024]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter qa_outputs.w_2.bias: torch.Size([2]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.self_attn.linears.0.weight: torch.Size([256, 256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.self_attn.linears.0.bias: torch.Size([256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.self_attn.linears.1.weight: torch.Size([256, 256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.self_attn.linears.1.bias: torch.Size([256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.self_attn.linears.2.weight: torch.Size([256, 256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.self_attn.linears.2.bias: torch.Size([256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.self_attn.linears.3.weight: torch.Size([256, 256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.self_attn.linears.3.bias: torch.Size([256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.feed_forward.w_1.weight: torch.Size([1024, 256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.feed_forward.w_1.bias: torch.Size([1024]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.feed_forward.w_2.weight: torch.Size([256, 1024]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.feed_forward.w_2.bias: torch.Size([256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.self_attn_norm.weight: torch.Size([256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.self_attn_norm.bias: torch.Size([256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.ff_norm.weight: torch.Size([256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter transformer_layer.ff_norm.bias: torch.Size([256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter doc_mlp.w_1.weight: torch.Size([1024, 256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter doc_mlp.w_1.bias: torch.Size([1024]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter doc_mlp.w_2.weight: torch.Size([1, 1024]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter doc_mlp.w_2.bias: torch.Size([1]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter sent_mlp.w_1.weight: torch.Size([1024, 256]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter sent_mlp.w_1.bias: torch.Size([1024]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter sent_mlp.w_2.weight: torch.Size([1, 1024]), require_grad = True
2020-11-06 13:03:34 INFO     Parameter sent_mlp.w_2.bias: torch.Size([1]), require_grad = True
2020-11-06 13:03:34 INFO     ***************************************************************************
2020-11-06 13:03:34 INFO     Model hype-parameter information...
2020-11-06 13:03:34 INFO     Hype-parameter	cuda = False
2020-11-06 13:03:34 INFO     Hype-parameter	do_debug = False
2020-11-06 13:03:34 INFO     Hype-parameter	do_train = True
2020-11-06 13:03:34 INFO     Hype-parameter	do_valid = True
2020-11-06 13:03:34 INFO     Hype-parameter	do_test = False
2020-11-06 13:03:34 INFO     Hype-parameter	do_retrieval = False
2020-11-06 13:03:34 INFO     Hype-parameter	evaluate_train = False
2020-11-06 13:03:34 INFO     Hype-parameter	data_path = /Users/xjtuwgt/PycharmProjects/sharedHotPotQARetrieval/data/hotpotqa/distractor_qa
2020-11-06 13:03:34 INFO     Hype-parameter	train_data_name = hotpot_train_distractor_wiki_tokenized.json
2020-11-06 13:03:34 INFO     Hype-parameter	train_data_filtered = 0
2020-11-06 13:03:34 INFO     Hype-parameter	dev_data_name = hotpot_dev_distractor_wiki_tokenized.json
2020-11-06 13:03:34 INFO     Hype-parameter	model = Unified QA Model
2020-11-06 13:03:34 INFO     Hype-parameter	pretrained_cfg_name = allenai/longformer-base-4096
2020-11-06 13:03:34 INFO     Hype-parameter	gpu_num = 4
2020-11-06 13:03:34 INFO     Hype-parameter	test_batch_size = 64
2020-11-06 13:03:34 INFO     Hype-parameter	gamma = 2.0
2020-11-06 13:03:34 INFO     Hype-parameter	alpha = 1.0
2020-11-06 13:03:34 INFO     Hype-parameter	model_name = MLP
2020-11-06 13:03:34 INFO     Hype-parameter	hop_model_name = DotProduct
2020-11-06 13:03:34 INFO     Hype-parameter	frozen_layer_num = 2
2020-11-06 13:03:34 INFO     Hype-parameter	pad_neg_samp_size = 8
2020-11-06 13:03:34 INFO     Hype-parameter	project_dim = 256
2020-11-06 13:03:34 INFO     Hype-parameter	global_mask_type = query_doc
2020-11-06 13:03:34 INFO     Hype-parameter	training_shuffle = 0
2020-11-06 13:03:34 INFO     Hype-parameter	sent_threshold = 0.85
2020-11-06 13:03:34 INFO     Hype-parameter	max_sent_num = 150
2020-11-06 13:03:34 INFO     Hype-parameter	max_ctx_len = 4096
2020-11-06 13:03:34 INFO     Hype-parameter	weight_decay = 1e-06
2020-11-06 13:03:34 INFO     Hype-parameter	learning_rate = 4e-05
2020-11-06 13:03:34 INFO     Hype-parameter	batch_size = 2
2020-11-06 13:03:34 INFO     Hype-parameter	input_drop = 0.1
2020-11-06 13:03:34 INFO     Hype-parameter	attn_drop = 0.1
2020-11-06 13:03:34 INFO     Hype-parameter	heads = 8
2020-11-06 13:03:34 INFO     Hype-parameter	with_graph = True
2020-11-06 13:03:34 INFO     Hype-parameter	span_weight = 0.1
2020-11-06 13:03:34 INFO     Hype-parameter	pair_score_weight = 0.2
2020-11-06 13:03:34 INFO     Hype-parameter	seq_project = True
2020-11-06 13:03:34 INFO     Hype-parameter	num_labels = 2
2020-11-06 13:03:34 INFO     Hype-parameter	grad_clip_value = 10.0
2020-11-06 13:03:34 INFO     Hype-parameter	cpu_num = 12
2020-11-06 13:03:34 INFO     Hype-parameter	init_checkpoint = None
2020-11-06 13:03:34 INFO     Hype-parameter	save_path = ../model
2020-11-06 13:03:34 INFO     Hype-parameter	max_steps = 271344
2020-11-06 13:03:34 INFO     Hype-parameter	epoch = 6
2020-11-06 13:03:34 INFO     Hype-parameter	warm_up_steps = 2000
2020-11-06 13:03:34 INFO     Hype-parameter	save_checkpoint_steps = 10000
2020-11-06 13:03:34 INFO     Hype-parameter	valid_steps = 1000
2020-11-06 13:03:34 INFO     Hype-parameter	log_steps = 50
2020-11-06 13:03:34 INFO     Hype-parameter	test_log_steps = 10
2020-11-06 13:03:34 INFO     Hype-parameter	rand_seed = 12345
2020-11-06 13:03:34 INFO     ***************************************************************************
2020-11-06 13:03:34 INFO     batch_size = 2
2020-11-06 13:03:34 INFO     projection_dim = 256
2020-11-06 13:03:34 INFO     learning_rate = 4e-05
2020-11-06 13:03:34 INFO     Start training...
2020-11-06 13:03:34 INFO     Starting warm up...
2020-11-06 13:03:34 INFO     ***************************************************************************
